{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Data preprocessing***\n",
    "\n",
    "\n",
    "## *В данном документе описан процесс предобработки исторических данных о теннисных матчах, полученных из [БД энтузиастов](https://github.com/JeffSackmann/tennis_atp \"Jeff Sackmann Github repository\")*\n",
    "\n",
    "\n",
    "## Skills changelog:\n",
    "*это первая попытка использования Machine Learning как инструмента в принципе, и поэтому\n",
    "- есть только общее понимание того, что из себя представляет Data Science в целом и Machine Learning в частности;\n",
    "- есть огромное желание попробовать ML на практике;\n",
    "- используются любые руководства для начинающих;\n",
    "- отсутствуют специфические знания и навыки, а именно - для подготовки данных, обучения моделей и т.п.;\n",
    "- все, чем приходится пока руководствоваться - логика и здравый смысл.\n",
    "\n",
    "\n",
    "## Sources:\n",
    "- Python 3 and Pandas docs;\n",
    "- StackOverflow\n",
    "\n",
    "***Примечание:*** изначально предобработка данных была реализована на Ruby (изучение Python находилось на самом начальном этапе, а желание приступить к проекту было уже давно). Однако, для удобства использования единого формата, в данной статье реализованы все те же шаги, только с использованием Python.\n",
    "\n",
    "\n",
    "\n",
    "# Разделы:\n",
    "## [1. Отбор турниров для анализа](#section1)\n",
    "## [2. Очистка данных](#section2)\n",
    "## [3. Непосредственная предпобработка данных](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале импортируем необходимые библиотеки и, для удобства в дальнейшем, зафиксируем корневой каталог репозитория:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### !pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import urllib\n",
    "import urllib.request\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "\n",
    "# Pandas display options tweaking\n",
    "pd.options.display.max_columns = None\n",
    "INIT_MAX_ROWS = 50\n",
    "pd.options.display.max_rows = INIT_MAX_ROWS\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "print('Root directory:', ROOT_DIR)\n",
    "# Downloaded files path\n",
    "DOWNL_F_P = os.path.join(ROOT_DIR, 'match_data', 'match_data_downloaded')\n",
    "# Preprocessed files path\n",
    "PREP_F_P = os.path.join(ROOT_DIR, 'match_data', 'match_data_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section1'></a>\n",
    "## 1. Отбор турниров для анализа\n",
    "### Для данного учебного проекта выбраны матчи Ассоциации теннисистов-профессионалов (ATP) (без учета квалификационных матчей - существует распространенное мнение, что в таких матчах игроки не демонстрируют весь свой потенциал, и, соответственно, данные об этих матчах могут носить дезинформирующий характер для ML-модели).\n",
    "\n",
    "Так выглядит [репозиторий](https://github.com/JeffSackmann/tennis_atp \"Jeff Sackmann Github repository\"), содержащий информацию о матчах разных годов и не только:\n",
    "![Содержание репозитория с информацией о теннисных матчах 1](../../img/notebooks/series_1/preprocessing/tennis-repo-overview-1.jpg \"Tennis repo overview\")\n",
    "...\n",
    "![Содержание репозитория с информацией о теннисных матчах 1](../../img/notebooks/series_1/preprocessing/tennis-repo-overview-2.jpg \"Tennis repo overview\")\n",
    "\n",
    "Таблицы содержат строки с информацией матча:\n",
    "- турнир (id турнира, название, покрытие, дата и др.);\n",
    "- игроки (id, имя, рост, возраст и др.);\n",
    "- очки (эйсы, двойные ошибки, подача с первой подачи, со второй, отыгранные брейк-поинты и др.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядит таблица с данными о матчах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_2007.csv'\n",
    "\n",
    "def show_csv(file):\n",
    "    data = pd.read_csv(file, header=0, encoding=\"utf-8-sig\", engine='python')    \n",
    "    display(data)\n",
    "    return data\n",
    "\n",
    "show_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если посмотреть таблицы за разные года, то можно увидеть, что данные по очкам за матч (крайние правые столбцы) начинают стабильно появляться в файлах с 1991 года по текущее время. Отчеты за эти года и будут использованы в данном проекте.\n",
    "\n",
    "Загрузим эти файлы в наш проект:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_raw_data_files():\n",
    "    cnt = 0\n",
    "    for year in range(1991, 2019):\n",
    "        output_template = os.path.join(DOWNL_F_P, 'atp_matches_')\n",
    "        extension = '.csv'\n",
    "        output = str(output_template + str(year) + extension)\n",
    "        url = 'https://raw.githubusercontent.com/JeffSackmann/tennis_atp/master/atp_matches_' + year + '.csv'\n",
    "        print('Downloading', url)\n",
    "        urllib.request.urlretrieve(url, output)\n",
    "        cnt += 1\n",
    "    print(cnt, 'files downloaded.')\n",
    "\n",
    "download_raw_data_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим все файлы в один CSV-файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_headers(path, file):\n",
    "    file_dir = os.path.join(path, file)\n",
    "    headers = ''\n",
    "    with open(file_dir, 'r', encoding='utf-8-sig') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)\n",
    "    return headers\n",
    "\n",
    "def _write_headers(file, flag, headers):\n",
    "    csv_merge = open(file, flag)\n",
    "    csv_merge.write(\",\".join(headers))\n",
    "    csv_merge.write('\\n')\n",
    "    csv_merge.close()\n",
    "    \n",
    "def _write_data(input_file, output_file, flag):\n",
    "    with open(input_file, 'r') as input, open(output_file, flag) as output:\n",
    "        reader = csv.reader(input)\n",
    "        writer = csv.writer(output, quotechar=None)\n",
    "        # skip first line with headers\n",
    "        next(reader)\n",
    "        for r in reader:\n",
    "            writer.writerow(r)\n",
    "            \n",
    "def get_files_in_directory(path, ext):\n",
    "    all_filenames = []\n",
    "    for root,dirs,files in os.walk(path):\n",
    "        # create a list of all CSVs with full path (i.e. directory)\n",
    "        func = lambda f: os.path.join(root, f)\n",
    "        all_filenames = sorted([func(file) for file in files if re.search(r'atp_matches_....\\.csv$', file)])\n",
    "    return all_filenames\n",
    "\n",
    "def merge_csv_files(path, input_files_list, output_name):\n",
    "    full_output = os.path.join(path, output_name)\n",
    "    headers = _get_headers(path, input_files_list[0])\n",
    "    \n",
    "    _write_headers(full_output, 'w', headers)\n",
    "    for file in input_files_list:\n",
    "        _write_data(file, full_output, 'a')\n",
    "\n",
    "        \n",
    "raw_match_data_files = get_files_in_directory(DOWNL_F_P, 'csv')\n",
    "merge_csv_files(DOWNL_F_P,\n",
    "                raw_match_data_files,\n",
    "                'atp_matches_1991-2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем, сначала не загружая весь файл в DataFrame, сколько итоговый файл содержит строк и столбцов и сколько занимает памяти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_match_data_file = 'atp_matches_1991-2018.csv'\n",
    "\n",
    "raw_data_full_path = os.path.join(DOWNL_F_P, raw_match_data_file)\n",
    "\n",
    "\n",
    "with open(raw_data_full_path, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    # read first line to count features\n",
    "    feature_count = len(next(reader))\n",
    "    # as we have already read first line (describing columns) there only left lines with match data\n",
    "    row_count = sum(1 for row in reader) - 1\n",
    "\n",
    "raw_data_file_size = os.path.getsize(raw_data_full_path) / 1024 / 1024\n",
    "print('File size:', raw_data_file_size, 'MB')\n",
    "print('Number of lines:', row_count)\n",
    "print('Number of features:', feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, файл не очень велик, поэтому можем работать с ним в DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = pd.read_csv(raw_data_full_path, header=0, encoding=\"utf-8\", engine='python')\n",
    "match_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем данные так, чтобы матчи шли от старых вначале к новым в конце.\n",
    "Сделать это можно упорядочив данные по:\n",
    "- дате турнира;\n",
    "- id турнира;\n",
    "- стадии турнира (от первых матчей в турнирной сетке до финала).\n",
    "\n",
    "Если с первыми двумя все понятно, то по третьему пункту нужно посмотреть, какие стадии вообще есть в таблице:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data['round'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Видно, что в каких-то записях отсутствует информация о стадии турнира. Посмотрим, что с этими матчами не так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data.loc[match_data['round'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, в этих строках отсутствует какая-либо информация - можем их удалить из таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use drop=True to avoid old index being added as a column\n",
    "match_data = match_data.dropna(how='all').reset_index(drop=True)\n",
    "match_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь убедимся, что со стадиями турнира все в порядке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_data_clean['round'].unique()\n",
    "match_data['round'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим оставшиеся записи по колонкам \"дата турнира\" и \"id турнира\" на отсутствие нулевых записей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data.loc[match_data['tourney_date'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data.loc[match_data['tourney_id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим записи о матчах групповых этапов турниров (RR) (вновь используя опцию drop=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_clean = match_data[match_data['round'] != 'RR'].reset_index(drop=True)\n",
    "match_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо отсортировать стадии турнира в соответствии с реальной последовательностью для дальнейшей\n",
    "сортировки всего DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tourney rounds sequence (BR - match for the third place)\n",
    "tourney_rounds = ['R128', 'R64', 'R32', 'R16', 'QF', 'SF', 'F', 'BR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем весь DataFrame в соответствии с выбранными критериями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 'round' column categorical\n",
    "match_data_clean['round'] = pd.Categorical(match_data_clean['round'], tourney_rounds)\n",
    "\n",
    "# Sort data by chosen features\n",
    "match_data_clean.sort_values(['tourney_date', 'tourney_id', 'round'],\n",
    "                             ascending=[True, True, True],\n",
    "                             inplace = True,\n",
    "                             na_position='first')\n",
    "match_data_clean = match_data_clean.reset_index(drop=True)\n",
    "match_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем небольшую перестановку в колонках для удобства проверки сортировки.\n",
    "\n",
    "Для этого сначала получим текущие колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = match_data_clean.columns.tolist()\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так будет выглядеть новый порядок колонок в таблице:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols_order = ['tourney_date', 'tourney_id', 'tourney_name', \n",
    "                  'round', 'surface', 'draw_size', 'tourney_level', \n",
    "                  'match_num', 'winner_id', 'winner_seed', \n",
    "                  'winner_entry', 'winner_name', 'winner_hand', \n",
    "                  'winner_ht', 'winner_ioc', 'winner_age', \n",
    "                  'winner_rank', 'winner_rank_points', 'loser_id', \n",
    "                  'loser_seed', 'loser_entry', 'loser_name', \n",
    "                  'loser_hand', 'loser_ht', 'loser_ioc', \n",
    "                  'loser_age', 'loser_rank', 'loser_rank_points', \n",
    "                  'score', 'best_of', 'minutes', \n",
    "                  'w_ace', 'w_df', 'w_svpt', \n",
    "                  'w_1stIn', 'w_1stWon', 'w_2ndWon', \n",
    "                  'w_SvGms', 'w_bpSaved', 'w_bpFaced', \n",
    "                  'l_ace', 'l_df', 'l_svpt', \n",
    "                  'l_1stIn', 'l_1stWon', 'l_2ndWon', \n",
    "                  'l_SvGms', 'l_bpSaved', 'l_bpFaced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_clean = match_data_clean.reindex(columns=new_cols_order)\n",
    "pd.options.display.max_rows = 200\n",
    "match_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернем настройки отображения к исходным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = INIT_MAX_ROWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним текущее состояние таблицы в CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_file = 'atp_matches_1991-2018_cleaned_reordered.csv'\n",
    "clean_data_full_path = os.path.join(DOWNL_F_P, clean_data_file)\n",
    "match_data_clean.to_csv(clean_data_full_path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь приступим к подготовке DataFrame с данными для обучения модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(match_data_clean['round'].unique())\n",
    "print(match_data_clean['surface'].unique())\n",
    "print(match_data_clean['best_of'].unique())\n",
    "print(match_data_clean['draw_size'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем названия колонок к единому формату:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cols = match_data_clean.columns.tolist()\n",
    "new_cols = []\n",
    "for col in old_cols:\n",
    "    if 'winner' in col:\n",
    "        col = col.replace('winner', 'w')\n",
    "    if 'loser' in col:\n",
    "        col = col.replace('loser', 'l')\n",
    "    new_cols.append(col)\n",
    "print(new_cols)\n",
    "\n",
    "match_data_clean.columns = new_cols\n",
    "match_data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним текущее состояние таблицы в CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_file = 'atp_matches_1991-2018_clean_final.csv'\n",
    "clean_data_full_path = os.path.join(DOWNL_F_P, clean_data_file)\n",
    "match_data_clean.to_csv(clean_data_full_path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим итоговый полученный CSV в DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_final_data_file = 'atp_matches_1991-2018_clean_final.csv'\n",
    "clean_data_full_path = os.path.join(DOWNL_F_P, clean_final_data_file)\n",
    "clean_data = pd.read_csv(clean_data_full_path, header=0, encoding=\"utf-8-sig\", engine='python')\n",
    "clean_data.iloc[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основная идея анализа:\n",
    "### Прогноз будет строиться на основе данных о предыдущих матчах игроков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем параметры, которые явно зависят от игроков (это первая проба выдвижения гипотезы - впоследствии этот список, безусловно, будет корректироваться):\n",
    "\n",
    "\n",
    "Для более удобного анализа нам понадобиться история матчей в порядке от последних к ранним. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_parse = ['w_ht', 'l_ht', 'w_age', 'l_age', 'w_rank', 'l_rank', 'w_rank_points', 'l_rank_points', 'w_ace', 'l_ace', 'w_df', 'l_df', 'w_svpt', 'l_svpt', 'w_1stIn', 'l_1stIn', 'w_1stWon', 'l_1stWon', 'w_2ndWon', 'l_2ndWon']\n",
    "# assume result columns as reciprocal of winner and loser parameters difference\n",
    "# max, min and avg - are maximum, minimum and average values respectively on the period or matches number chosen\n",
    "res_cols = ['ht', 'age', 'rank', 'rank_points', 'ace_last', 'ace_max', 'ace_mean', 'ace_min', 'df_last', 'df_max', 'df_mean', 'df_min', 'perc_1stIn_last', 'perc_1stIn_max', 'perc_1stIn_mean', 'perc_1stIn_min', 'perc_w_1stIn_last', 'perc_w_1stIn_max', 'perc_w_1stIn_mean', 'perc_w_1stIn_min', 'perc_w_2ndIn_last', 'perc_w_2ndIn_max', 'perc_w_2ndIn_mean', 'perc_w_2ndIn_min', 'winner_flag']\n",
    "\n",
    "def _get_date(value, format='%Y%m%d'):\n",
    "    date = datetime.strptime(str(int(value)), format)\n",
    "    return date\n",
    "\n",
    "def _get_df_cell_value(df, row_index, col):\n",
    "    return df.iloc[[row_index]][col]\n",
    "\n",
    "def _percent(part, whole):\n",
    "    if float(whole) == 0 : return 0\n",
    "    perc = float(part) * 100 / float(whole)\n",
    "    return round(perc, 4)\n",
    "\n",
    "def _get_values_list(feature, df, prefixes):\n",
    "    values = []\n",
    "    for i, row in df.iterrows():\n",
    "        column = prefixes[i] + feature\n",
    "        values.append(row[column])\n",
    "    return values\n",
    "\n",
    "def _last(feature, df, prefixes):\n",
    "    column = prefixes[0] + feature\n",
    "    res = round( int( df.iloc[0].loc[column] ), 4 )\n",
    "    return res\n",
    "\n",
    "def _min(feature, df, prefixes):\n",
    "    res = round( int( min( _get_values_list( feature, df, prefixes ) ) ), 4 )\n",
    "    return res\n",
    "\n",
    "def _max(feature, df, prefixes):\n",
    "    res = round( int( max( _get_values_list( feature, df, prefixes ) ) ), 4 )\n",
    "    return res\n",
    "\n",
    "def _mean(values_list):\n",
    "    res = round( sum( values_list ) / float( len( values_list ) ), 4 )\n",
    "    return res\n",
    "\n",
    "def _check_features(row, features):\n",
    "    for f in features:\n",
    "        if np.isnan(row[f]):\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def _check_for_valid_matches(df, init_index, player_name, **params):\n",
    "    valid_matches_indices = []\n",
    "    init_date = _get_date(_get_df_cell_value(df, init_index, 'tourney_date'))\n",
    "    row_index = init_index + 1\n",
    "    \n",
    "    while row_index < df.shape[0]:\n",
    "        row = df.iloc[row_index]        \n",
    "        valid_date = (init_date - _get_date(row['tourney_date'])).days <= params.get('period')\n",
    "        valid_count = len(valid_matches_indices) <= params.get('num_max')\n",
    "        valid_name = (player_name == row['w_name']) | (player_name == row['l_name'])\n",
    "        \n",
    "        if valid_date & valid_count:\n",
    "            if valid_name:\n",
    "                if _check_features(row, params.get('cols_to_parse')):\n",
    "                    valid_matches_indices.append(row_index)\n",
    "                else:\n",
    "                    break\n",
    "            row_index += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if len(valid_matches_indices) < params.get('num_min'):\n",
    "        valid_matches_indices = []\n",
    "\n",
    "    return valid_matches_indices\n",
    "\n",
    "def _get_valid_matches(df, init_index, player_name, **params):\n",
    "    valid_matches_indices = _check_for_valid_matches(df, init_index, player_name, **params)\n",
    "    p1ayer_valid_matches = pd.DataFrame(columns=df.columns.tolist())\n",
    "    \n",
    "    if valid_matches_indices:\n",
    "        for i, row_index in enumerate(valid_matches_indices):\n",
    "            p1ayer_valid_matches.loc[i] = df.iloc[row_index]\n",
    "\n",
    "    p1ayer_valid_matches.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return p1ayer_valid_matches\n",
    "\n",
    "def _get_prefixes(name, df):\n",
    "    prefixes = []\n",
    "    for i,row in df.iterrows():\n",
    "        prefix = 'w_' if name == row['w_name'] else 'l_'\n",
    "        prefixes.append(prefix) \n",
    "    \n",
    "    return prefixes\n",
    "\n",
    "def _add_features(df, prefixes):\n",
    "    features_to_derive = ('perc_1stIn',\n",
    "                           'perc_w_1stIn',\n",
    "                           'perc_w_2ndIn')\n",
    "    \n",
    "    for feature in features_to_derive:\n",
    "        df['w_' + feature] = 0.0\n",
    "        df['l_' + feature] = 0.0\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i]\n",
    "        for pr in ('w_', 'l_'):\n",
    "            df.loc[i, pr + 'perc_1stIn'] = _percent(row.loc[pr + '1stIn'], row.loc[pr + 'svpt'])\n",
    "            df.loc[i, pr + 'perc_w_1stIn'] = _percent(row.loc[pr + '1stWon'], row.loc[pr + '1stIn'])\n",
    "            df.loc[i, pr + 'perc_w_2ndIn'] = _percent(row.loc[pr + '2ndWon'], row.loc[pr + 'svpt'] - row.loc[pr + '1stIn'])\n",
    "\n",
    "def _preprocess_player_valid_matches(player_name, df):\n",
    "    res = {}\n",
    "    prefixes = _get_prefixes(player_name, df)\n",
    "    features_to_copy = ('ht',\n",
    "                        'age',\n",
    "                        'rank',\n",
    "                        'rank_points')\n",
    "    features_compound = ('ace',\n",
    "                         'df',\n",
    "                         'perc_1stIn',\n",
    "                         'perc_w_1stIn',\n",
    "                         'perc_w_2ndIn')\n",
    "    \n",
    "    # hard-coded derivative features addition\n",
    "    _add_features(df, prefixes)\n",
    "    \n",
    "    for feature in features_to_copy:\n",
    "        res[feature] = int(df.iloc[0].loc[prefixes[0] + feature])\n",
    "    \n",
    "    for feature in features_compound:\n",
    "        res[feature + '_last'] = _last(feature, df, prefixes)\n",
    "        res[feature + '_min'] = _min(feature, df, prefixes)\n",
    "        res[feature + '_max'] = _max(feature, df, prefixes)\n",
    "        res[feature + '_mean'] = _mean(_get_values_list(feature, df, prefixes))\n",
    "        \n",
    "    return res\n",
    "\n",
    "def _get_res(player_one_data, player_two_data, flag):\n",
    "    res = {}\n",
    "\n",
    "    for feature in player_one_data.keys():\n",
    "        diff = int(player_one_data[feature]) - int(player_two_data[feature])\n",
    "        if diff == 0:\n",
    "            res[feature] = 0\n",
    "        else:\n",
    "            res[feature] = round(1 / float(diff), 4)\n",
    "    \n",
    "    res['winner_flag'] = int(flag)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def _preprocess_result_row(df_list, index):\n",
    "    res_row = {}\n",
    "    \n",
    "    if index % 2 == 0:\n",
    "        res_row = _get_res(df_list[0], df_list[1], 1)\n",
    "    else:\n",
    "        res_row = _get_res(df_list[1], df_list[0], -1)\n",
    "    return res_row\n",
    "\n",
    "def _get_res_row_values(res_row_dict, cols):\n",
    "    res_list = []\n",
    "\n",
    "    for i,col in enumerate(cols):\n",
    "        res_list.insert(i, res_row_dict[col])\n",
    "    return res_list\n",
    "\n",
    "def preprocess(df, **params):\n",
    "    print('Preprocessing match data...')\n",
    "    res_frame = pd.DataFrame(columns=res_cols)\n",
    "    rows_written = 0\n",
    "    res_curr_index = 0\n",
    "    \n",
    "    for init_index, init_row in df.iterrows():\n",
    "        if init_index % 100 == 0:\n",
    "            print('Preprocessing row:    ', init_index, '   Rows written:    ', res_curr_index)\n",
    "\n",
    "        valid_matches = []\n",
    "        players = [init_row['w_name'], init_row['l_name']]\n",
    "        \n",
    "        for player_name in players:\n",
    "            valid_matches.append( _get_valid_matches( df, init_index, player_name, **params ) )\n",
    "        \n",
    "        # if there is a lack of information on any of features being analysed \n",
    "        if valid_matches[0].empty | valid_matches[1].empty:\n",
    "            continue\n",
    "\n",
    "        preprocessed_player_data = []\n",
    "\n",
    "        for i,player_name in enumerate(players):\n",
    "            prep_valid_matches = _preprocess_player_valid_matches(player_name, valid_matches[i])\n",
    "            preprocessed_player_data.insert(i, prep_valid_matches)\n",
    "        \n",
    "        res_row_dict = _preprocess_result_row(preprocessed_player_data, init_index)\n",
    "\n",
    "        res_row = _get_res_row_values(res_row_dict, res_cols)\n",
    "        res_frame.loc[res_curr_index] = res_row\n",
    "        res_curr_index += 1\n",
    "        \n",
    "        if res_curr_index % params.get('rows_to_write') == 0:\n",
    "            preprocessed_data_file = 'atp_matches_1991-2018_preprocessed.csv'\n",
    "            preprocessed_data_full_path = os.path.join(preprocessed_files_path, preprocessed_data_file)\n",
    "            res_frame.to_csv(preprocessed_data_full_path, encoding='utf-8-sig', index=False)\n",
    "    \n",
    "    res_frame.winner_flag = res_frame.winner_flag.astype(int)\n",
    "    return res_frame\n",
    "\n",
    "# Preprocessing parameters are:\n",
    "# - period in days during which previous matches are analysed\n",
    "# - minimum number of matches to analyse\n",
    "# - maximum number of matches to analyse\n",
    "preprocess_params = {'rows_to_write':100,\n",
    "                     'period':14,\n",
    "                     'num_min':3,\n",
    "                     'num_max':10,\n",
    "                     'cols_to_parse':cols_to_parse,\n",
    "                     'res_cols': res_cols}\n",
    "reverse_clean_data = clean_data.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "%time preprocessed_data = preprocess(reverse_clean_data, **preprocess_params)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем результат в отдельный CSV-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_file = 'atp_matches_1991-2018_preprocessed_final.csv'\n",
    "preprocessed_data_full_path = os.path.join(PREP_F_P, preprocessed_data_file)\n",
    "preprocessed_data.to_csv(preprocessed_data_full_path, encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
